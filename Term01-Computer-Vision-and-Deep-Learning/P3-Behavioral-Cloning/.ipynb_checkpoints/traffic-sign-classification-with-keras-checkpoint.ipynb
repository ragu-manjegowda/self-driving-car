{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import pandas\n",
    "import csv\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cutting the image to the section, that holds the road information\n",
    "def cut_images_to_arr(img_Center):\n",
    "    arr_Center = np.array(img_Center)\n",
    "    arr_Center = arr_Center[50:]\n",
    "    return arr_Center\n",
    "\n",
    "#Converting the RGB Image to an HLS Image\n",
    "def convert_to_HLS(img):\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    return hls\n",
    "\n",
    "#Normalizing the input Image\n",
    "def normalize_image(image_data):\n",
    "    a = 0.01\n",
    "    b = 0.99\n",
    "    color_min = 0.0\n",
    "    color_max = 255.0\n",
    "    return a + ( ( (image_data - color_min) * (b - a) )/(color_max - color_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['center', 'left', 'right', 'steering', 'throttle', 'brake', 'speed']\n",
      "6788885515\n",
      "Pickle Train_data\n",
      "Train_data in Pickle File.\n"
     ]
    }
   ],
   "source": [
    "#Reading the driving log to match stearing information to Images\n",
    "with open('./data/driving_log.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    driving_list = list(reader)\n",
    "    \n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "#Preprocess all Images with cut/convert to HLS/Normalize\n",
    "for i, row in enumerate(driving_list):\n",
    "    if i == 0:\n",
    "        print(row)\n",
    "        continue\n",
    "    #print(row)\n",
    "    img_Center = Image.open('./data/' + row[0])\n",
    "\n",
    "    img_Center = cut_images_to_arr(img_Center)\n",
    "    #img_Center = convert_to_HLS(img_Center)\n",
    "    img_Center = normalize_image(img_Center)\n",
    "\n",
    "    X_train.append(img_Center)\n",
    "    y_train.append(row[3])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "#shuffle and split Training Data into Train and Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.2)\n",
    "\n",
    "#Pickle Data Training and Validation Data to make reuse of it.\n",
    "pickle_data = pickle.dumps(\n",
    "    {\n",
    "        'train_dataset': X_train,\n",
    "        'train_labels': y_train,\n",
    "        'val_dataset': X_val,\n",
    "        'val_labels': y_val\n",
    "    }, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "del X_train, X_val, y_train, y_val\n",
    "\n",
    "pickle_size = sys.getsizeof(pickle_data)\n",
    "print(pickle_size)\n",
    "\n",
    "# Save the data for easy access\n",
    "pickle_file = 'train_data.pickle'\n",
    "exists = False\n",
    "\n",
    "max_bytes = 2 ** 31 - 1\n",
    "\n",
    "#Cut down Data to smaller protions, since pickle cant handle data bigger than 2**31-1 bytes.\n",
    "while not exists:\n",
    "    if not os.path.isfile(pickle_file):\n",
    "        print('Pickle Train_data')\n",
    "        try:\n",
    "            with open(pickle_file, 'wb') as p_train_data:\n",
    "                for idx in range(0, pickle_size, max_bytes):\n",
    "                    p_train_data.write(pickle_data[idx:idx + max_bytes])\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', pickle_file, ':', e)\n",
    "            raise\n",
    "\n",
    "        print('Train_data in Pickle File.')\n",
    "        exists = True\n",
    "    else:\n",
    "        print(\"Pickle Filename already in use. Choose another name: *.pickle\")\n",
    "        pickle_file = input(\"Enter: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Sequential Model\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Create the Sequential model\n",
    "model = Sequential()\n",
    "```\n",
    "The `keras.models.Sequential` class is a wrapper for the neural network model. Just like many of the class models in scikit-learn, it provides common functions like `fit()`, `evaluate()`, and `compile()`.  We'll cover these functions as we get to them.  Let's start looking at the layers of the model.\n",
    "\n",
    "## Keras Layer\n",
    "A Keras layer is just like a neural network layer.  It can be fully connected, max pool, activation, etc.  You can add a layer to the model using the model's `add()` function.  For example, a simple model would look like this:\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "\n",
    "# Create the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Layer - Add a flatten layer\n",
    "model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "\n",
    "# 2nd Layer - Add a fully connected layer\n",
    "model.add(Dense(100))\n",
    "\n",
    "# 3rd Layer - Add a ReLU activation layer\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 4th Layer - Add a fully connected layer\n",
    "model.add(Dense(60))\n",
    "\n",
    "# 5th Layer - Add a ReLU activation layer\n",
    "model.add(Activation('relu'))\n",
    "```\n",
    "Keras will automatically infer the shape of all layers after the first layer.  This means you only have to set the input dimensions for the first layer.\n",
    "\n",
    "The first layer from above, `model.add(Flatten(input_shape=(32, 32, 3)))`, sets the input dimension to (32, 32, 3) and output dimension to (3072=32\\*32\\*3).  The second layer takes in the output of the first layer and sets the output dimenions to (100).  This chain of passing output to the next layer continues until the last layer, which is the output of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network\n",
    "\n",
    "1. Compile the network using adam optimizer and categorical_crossentropy loss function.\n",
    "2. Train the network for ten epochs and validate with 20% of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6428 train samples\n",
      "1608 test samples\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 22, 64, 60)    4560        convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 22, 64, 60)    14460       convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 11, 32, 60)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 11, 32, 60)    14460       maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 6, 16, 60)     0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 6, 16, 60)     129660      maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 3, 8, 60)      0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 3, 8, 60)      0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 1440)          0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden1 (Dense)                  (None, 40)            57640       flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 40)            0           hidden1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "hidden2 (Dense)                  (None, 20)            820         activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 20)            0           hidden2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "hidden3 (Dense)                  (None, 10)            210         activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 10)            0           hidden3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 10)            0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "Steering_Angle (Dense)           (None, 1)             11          dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 221,821\n",
      "Trainable params: 221,821\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6428 samples, validate on 1608 samples\n",
      "Epoch 1/35\n",
      "6428/6428 [==============================] - 140s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 2/35\n",
      "6428/6428 [==============================] - 106s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0168 - val_acc: 0.5442\n",
      "Epoch 3/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 4/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 5/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 6/35\n",
      "6428/6428 [==============================] - 104s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 7/35\n",
      "6428/6428 [==============================] - 107s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 8/35\n",
      "6428/6428 [==============================] - 106s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0168 - val_acc: 0.5442\n",
      "Epoch 9/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 10/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 11/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 12/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 13/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 14/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 15/35\n",
      "6428/6428 [==============================] - 104s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 16/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 17/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 18/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 19/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 20/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0168 - val_acc: 0.5442\n",
      "Epoch 21/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 22/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 23/35\n",
      "6428/6428 [==============================] - 104s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 24/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 25/35\n",
      "6428/6428 [==============================] - 104s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0168 - val_acc: 0.5442\n",
      "Epoch 26/35\n",
      "6428/6428 [==============================] - 104s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 27/35\n",
      "6428/6428 [==============================] - 104s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 28/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 29/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 30/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 31/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 32/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0168 - val_acc: 0.5442\n",
      "Epoch 33/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 34/35\n",
      "6428/6428 [==============================] - 104s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n",
      "Epoch 35/35\n",
      "6428/6428 [==============================] - 103s - loss: 0.0166 - acc: 0.5423 - val_loss: 0.0167 - val_acc: 0.5442\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'train_data.pickle'\n",
    "\n",
    "bytes_in = bytearray(0)\n",
    "max_bytes = 2 ** 31 - 1\n",
    "input_size = os.path.getsize(pickle_file)\n",
    "\n",
    "with open(pickle_file, 'rb') as p_train_data:\n",
    "    for _ in range(0, input_size, max_bytes):\n",
    "        bytes_in += p_train_data.read(max_bytes)\n",
    "pickle_data = pickle.loads(bytes_in)\n",
    "X_train = pickle_data['train_dataset']\n",
    "y_train = pickle_data['train_labels']\n",
    "X_val = pickle_data['val_dataset']\n",
    "y_val = pickle_data['val_labels']\n",
    "del pickle_data  # Free up memory\n",
    "\n",
    "batch_size = 100\n",
    "nb_classes = 1\n",
    "nb_epoch = 35\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_val.astype('float32')\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_val.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "#---Model-Definition:\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Start wird 4 Convolutiional Layers to recognize the image\n",
    "model.add(Convolution2D(60, 5, 5, subsample=(5, 5), border_mode='same', input_shape=input_shape, activation='relu', dim_ordering='tf'))\n",
    "model.add(Convolution2D(60, 2, 2, border_mode='same', input_shape=input_shape, activation='relu', dim_ordering='tf'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), border_mode='same', dim_ordering='tf'))\n",
    "model.add(Convolution2D(60, 2, 2, border_mode='same', input_shape=input_shape, activation='relu', dim_ordering='tf'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), border_mode='same', dim_ordering='tf'))\n",
    "model.add(Convolution2D(60, 6, 6, border_mode='same', input_shape=input_shape, activation='relu', dim_ordering='tf'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), border_mode='same', dim_ordering='tf'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Flatten the Matrix to a Vektor and run 3 RELU Layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(40, name=\"hidden1\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(20, name=\"hidden2\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10, name=\"hidden3\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, name=\"Steering_Angle\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, validation_data=(X_val, y_val))\n",
    "\n",
    "json_string = model.to_json()\n",
    "with open('./model.json', 'w') as outfile:\n",
    "    json.dump(json_string, outfile)\n",
    "\n",
    "model.save_weights('./model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Validation Accuracy:** 0.9911"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
